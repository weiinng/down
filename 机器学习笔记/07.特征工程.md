数据的特征抽取：
将文本等数据进行特征值化(转换成计算机可以理解的数字类型)
在使用文本数据来搭建预测模型前，都需要特殊的准备工作。
文本首先要通过解析来提取单词，这一过程称为词条化。
然后单词需要编码为整数或浮点值，作为机器学习算法的输入，称为特征提取(或量化)。

CountVectorizer：	将文本的转化成单词频数向量（量化单词数量）
CountVectorizer不仅可以将文本文档的数据集转化成词条并建立一个已知单词的词汇表，
而且还可以用该词汇表对新文本进行编码。

TfidfVectorizer：	提取文本的单词权重向量（计算单词权重）
TfidfVectorizer可以词条化文档，学习词汇表以及逆文档频率权重，并且可以编码新文档。
或者，如果你已经用CountVectorizer得到了向量，你可以对它使用Tfidftransformer函数，
计算逆文档频率并且开始编码文件。

HashingVectorizer：	将文本映射到特征索引（哈希量化文本）。
单词频率和权重是很有用的，但是当词汇表变得很大时，以上两种方法就会出现局限性。
反过来，这将需要巨大的向量来编码文档，并对内存要求很高，而且会减慢算法的速度。

一种很好的方法是使用单向哈希方法来将单词转化成整数。
这种量化方法不要求调用函数来对训练数据文件进行拟合。相反，在实例化之后，它可以直接用于编码文档。

好处是该方法不需要词汇表，可以选择任意长的固定长度向量。
缺点是哈希量化是单向的，因此无法将编码转换回单词(对许多有监督的学习任务来说并不重要)。



"""
字典特征提取
"""
from sklearn.feature_extraction import DictVectorizer      
	# 机器学习工具包下的特征提取模块中的字典特征提取模块
# 使用DictVectorizer对使用字典存储的数据进行特征抽取和矢量化
# DictVectorizer对非数字化的处理方式是，借助原特征的名称，组合成新的特征，并采用0/1的方式进行矢量化。

dv = DictVectorizer()              # 初始化一个字典特征处理器 参数sparse=False 把坐标转成高维数组(默认为True)

data = dv.fit_transform([{'city':'beijing','temperture':30},         # .fit_transform()方法 训练，转换并返回 
                         {'city':'shanghai','temperture':35},
                         {'city':'guangzhou','temperture':33}])      # 城市 温度
  # fit()方法的主要工作是获取特征信息和目标值信息，在这点上fit方法和模型训练时的fit方法就能够联系在一起了
  # 都是通过分析特征和目标值提取有价值的信息，对于转换类来说是某些统计量，对于模型来说可能是特征的权值系数等
  # fit_transform()方法对数据先fit训练拟合出模型,然后进行转换 相当于把一种数据转换为另一种矢量化数据

# print(data)                                            # 坐标与坐标值
# print(data.todense())                                                
	# 把坐标转成高维数组(矩阵类型)
# print(data.toarray())                                                
	# .toarray()方法理解为：编码过程 把坐标转成高维数组 总特征数量 beijing 30 
# print(dv.inverse_transform([[1,0,0,30]]))                            
	# .inverse_transform()方法理解为：解码过程 取出坐标对应数据
# print(dv.transform([{'city':'beijing','temperture':30}]).toarray())  
	# 经过训练后的DictVectorizer()不需要再次训练直接使用transform()转换就可以提取
# print(dv.get_feature_names())                          # 提取特征标题
# print(dv.vocabulary_)                                  # 获取词频 （特征列：对应特征标题所在下标索引位置）




"""
文本特征提取
"""
# 提取英文特征
from sklearn.feature_extraction.text import CountVectorizer    
	# 机器学习工具包下的特征提取模块下的文本模块中的文本特征提取模块

data = ["Life is short,i like Python","Life is too long,i dislike Python",'java php c++','i is like money']
cv = CountVectorizer()            # 初始化一个文本特征处理器 
  # 参数 vocabulary=['is'] 构建关键词集提取想要的特征文本 
  # stop_words=['too'] 设置停用词排除不想要的特征文本 
  # lowercase=True 所有特征词都变为小写默认为True
data = cv.fit_transform(data)     # 训练，转换并返回
print(cv.get_feature_names())     # 提取特征标题
print(cv.vocabulary_)             # 获取词频（特征列：对应特征标题所在下标位置）
print(data.toarray())             # 把坐标转成高维数组(ndarray类型)
print(data.todense())             # 把坐标转成高维数组(矩阵类型)
print(data)                       # 坐标与坐标值




# 计算欧氏距离
from sklearn.metrics.pairwise import euclidean_distances     
	# 欧氏距离模块(两点向量之间的距离) 距离越大样本的相似度越低

dist1 = euclidean_distances(X=data[0],Y=data[1])    # 初始化一个欧氏距离处理器 计算第一个与第二个数据的距离    
dist2 = euclidean_distances(X=data[1],Y=data[2])    # 计算第二个与第三个数据的距离
print(dist1)
print(dist2)




# 提取中文特征
import jieba                                                          # 分词模块
def cutword():
    contetn1 = jieba.lcut("我学会了python，java和php")
#     print(contetn1)
    contetn2 = jieba.lcut("python包括flask，django和数据库")
    contetn3 = jieba.lcut("java和php要求熟练掌握数据库")
    c1 = ' '.join(contetn1)                                           # 将分词以空格隔开
#     print(c1)
    c2 = ' '.join(contetn2)
    c3 = ' '.join(contetn3)
    return c1,c2,c3
print(cutword())

def getFeature():
    count = CountVectorizer(stop_words=['要求','学会','包括','熟练掌握'])   
	# 初始化一个文本特征提取器，并设置停用词
    a1, a2, a3 = cutword()                                             # 变量接收并调用分词的函数
#     print(a1)
#     print(a2)
#     print(a3)
    data = count.fit_transform([a1, a2, a3])                           # 训练，转换并返回
    print(count.get_feature_names())                                   # 提取特征标题
    print(data.toarray())                                              # 把坐标转成高维数组(ndarray类型)
getFeature()




# tf-idf文本特征提取
from sklearn.feature_extraction.text import TfidfVectorizer     # tf-idf（词频计算方法）文本特征提取模块
    # TF意思是词频，IDF意思是逆文本频率指数
    # 某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。
    # TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
    # 某个词对文章的重要性越高，它的TF-IDF值就越大
    
def tfidfvec():
    tfidf = TfidfVectorizer()                             # 初始化一个tf-idf文本特征提取器
    b1, b2, b3 = cutword()                                # 变量接收并调用分词的函数
    data = tfidf.fit_transform([b1, b2, b3])              # 训练，转换并返回
    print(tfidf.get_feature_names())                      # 提取特征标题
    print(data.toarray())                                 # 把坐标转成高维数组(ndarray类型)
    print(tfidf.vocabulary_)                              # 获取词频（特征列：对应特征标题所在下标位置）
tfidfvec()    

# 词频（term frequency，tf）指的是某一个给定的词语在'该文件中出现的频率'。                       
# 频率越高该词在文件中出现的次数越多
# 逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量(不同词重要性的度量)。 
# 度量越低该词在所有文件中出现的次数越多
# 某一特定词语的idf，可以由'总文件数目除以包含该词语之文件的数目'，再将得到的商，取以10为底的对数得到
# log是以n为底m的对数如log m9 n3= 次方2（3*3）  lg为以10为底m的对数如lg（10,000,000 / 1,000）=4




# 哈希算法矢量化文本特征提取
from sklearn.feature_extraction.text import HashingVectorizer           
	# HashingVectorizer（哈希算法矢量化）文本特征提取
	# 哈希算法不可逆无法通过解码转换对应文本
hashV = HashingVectorizer(n_features=5,stop_words=['is'])                 # 参数 n_features=5 只提取5个特征 
hash_V_data = hashV.fit_transform(['java is good','python is very good'])  
print(hash_V_data)
print(hash_V_data.toarray())




# 文本特征词频统计
from collections import Counter                           # 收集模块中的统计模块
import jieba                                              # 分词模块

word_list1 = jieba.lcut('我们我们中国人厉害')             # .lcut()分词方法  返回的是一个列表
print(word_list1)

word_list2 = jieba.lcut('我们中国人的python厉害')
print(word_list2)

word_list1.extend(word_list2)                             # 合并两个jieba
print(word_list1)

counter = Counter(word_list1)                             # 初始化一个统计器
dictionary=dict(counter)                                  # 转换dict用于显示词频个数
print(dictionary)                                         # 统计词频个数
print(counter.most_common(2))                             # most_common(2)取出前2个最大的词频




