矢量：
是一种既有大小又有方向的量，又称为向量，通常被标示为一个带箭头的线段。线段的长度可以表示矢量的大小，而矢量的方向也就是箭头所指的方向。与矢量概念相对的是只有大小而没有方向的标量。



MEDV：
有两个主要的应用：初始化对象和动态更新



多元回归：
	【什么是'回归'	回归就是向平均靠拢。】
    多元回归是指一个因变量（预报对象）,多个自变量（预报因子）的回归模型。这种包括两个或两个以上自变量的回归称为多元回归。
    多元回归可以加深对定性分析结论的认识，并得出各种要素间的数量依存关系，从而进一步揭示出各要素间内在的规律。一般来说，多元回归过程能同时提供多个备选的函数关系式，并提供每个关系式对实验数据的理解能力。回归分析的任务就是用数学表达式来描述相关变量之间的关系。

回归分析的基本思想是：
    虽然因变量和自变量之间没有严格的、确定性的函数关系（完全确定关系），但可以设法找出最能代表它们之间关系的数学表达式（相关关系）。

多元线性回归分析主要解决以下几方面的问题：
1.确定几个特定的变量之间是否存在相关关系，如果存在的话，找出它们之间合适的数学表达式。

2.根据一个或几个变量的值，预测或控制另一个变量的取值，并且可以知道这种预测或控制能达到什么样的精确度。

3.进行因素分析。例如在对于共同影响一个变量的许多变量（因素）之间，找出哪些是重要因素，哪些是次要因素，这些因素之间又有什么关系等等。

注意点：
	线性回归方程纳入的自变量越多，越应该能够反应现实，但解释起来就越困难。
	多元线性回归分析的主要目的是：	解释和预测




决策树：
    回归树--对连续变量做决策树
    分类树--对离散变量做决策树
是一种常见的用于分类和回归的非参数监督学习方法，目标是创建一个模型，通过从数据特性中推导出简单的决策规则来预测目标变量的值。

    决策点(内部节点)：	表示一个特征或属性
	是对几种决策方案的选择，即最后选择的最佳方案。
	如果决策属于多级决策，则决策树的中间可以有多个决策点，以决策树根部的决策点为最终决策方案。
    状态节点(叶节点)：	表示一个类
	代表备选方案的经济效果（期望值），通过各状态节点的经济效果的对比，按照一定的决策标准就可以选出最佳方案。
	由状态节点引出的分支称为概率枝，概率枝的数目表示可能出现的自然状态数目每个分枝上要注明该状态出现的概率。
    结果节点：
	将每个方案在各种自然状态下取得的损益值标注于结果节点的右端。

优点：
    决策树易于理解和实现，在学习过程中不需要使用者了解很多的背景知识，它能够直接体现数据的特点，只要通过解释后人们都有能力去理解决策树所表达的意义。
    决策树数据的准备往往是简单或者是不必要的，而且能够同时处理数据型和常规型属性，在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
    易于通过静态测试来对模型进行评测，可以测定模型可信度。如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。

缺点：
    对连续性的字段比较难预测。
    对有时间顺序的数据，需要很多预处理的工作。
    当类别太多时，错误可能就会增加的比较快。
    一般的算法分类的时候，只是根据一个字段来分类。
    容易过拟合，决策树学习可能会生成过于复杂的数结构，不能代表普遍的规则。
    决策树可能不稳定，因为数据中的小变化可能导致生成完全不同的树。
    对于各类别样本数量不一致的数据, 信息增益偏向于那些更多数值的特征。
    忽略属性之间的相关性。

过拟合：
是指为了得到一致假设而使假设变得过度严格，通常采用增大数据量和测试样本集的方法对分类器性能进行评价。

分类树：
是一种十分常用的分类方法。就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类，这样的机器学习被称之为监督学习。
	【恩...描述对实例进行分类的树形结构，预测分析模型。】

使用决策树做预测的过程：
收集数据，准备数据，分析数据，训练算法，测试算法，使用算法

    使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。
采用递归的方法进行建树


代码思路：
BuildTree函数	：在该函数中完成递归建树，递归返回条件的判断，建立存储树所用的字典，打印各类信息
ChooseAttr函数	：在该函数中完成选出最佳特征的功能，根据Ent函数计算出的所有样本的信息熵和加权的信息熵计算信息增益，信息增越大的意味着该属性的纯度越高，选取信息增益最大的属性为最佳属性。
Ent函数		：计算输入样本的信息熵，通过输入Sample的最后一列统计出该正例与反例出现的概率，根据信息熵公式计算信息熵
SpiltData函数	：该函数用于对数据进行拆分，去掉已经判断过的属性对应的样本
CreatePlot函数	：用于决策树的可视化




有监督有标签有分类与回归
无监督无标签叫聚类




sklearn模块：机器学习工具包
scikit-learn简称sklearn，支持包括分类、回归、降维和聚类四大机器学习算法。还包含了特征提取、数据处理和模型评估三大模块。
sklearn是Scipy的扩展，建立在NumPy和matplotlib库的基础上。利用这几大模块的优势，可以大大提高机器学习的效率。
sklearn拥有着完善的文档，上手容易，具有着丰富的API。sklearn已经封装了大量的机器学习算法，包括LIBSVM和LIBINEAR。同时sklearn内置了大量数据集，节省了获取和整理数据集的时间。

特征提取：
我们获取的数据中很多数据往往有很多维度，但并不是所有的维度都是有用的，有意义的，所以我们要将对结果影响较小的维度舍去，保留对结果影响较大的维度。
PCA（主成分分析）与LDA（线性评价分析）是特征提取的两种经典算法。PCA与LDA本质上都是学习一个投影矩阵，使样本在新的坐标系上的表示具有相应的特性，样本在新坐标系的坐标相当于新的特征，保留下的新特征应当是对结果有较大影响的特征。

主要内容包括：
1.PCA算法
2.LDA算法
3.线性回归
4.逻辑回归
5.朴素贝叶斯
6.决策树
7.SVM
8.神经网络
9.KNN算法

基本功能主要被分为六大部分：
分类任务
回归任务
聚类任务
数据降维  （应用于如果每个维度有大数据量时使用）
模型选择  （理解为算法）
数据预处理（均值移除，范围缩放，归一化，二值化，独热编码，数据离散化）


常用的数据集：		
	from sklearn import datasets		机器学习工具包中的数据集模块
1.自带的小数据集	
    使用方法		：sklearn.datasets.load_<name>
    鸢尾花数据集		：load_iris（）		：用于分类任务
    手写数字数据集		：load_digits（）	：用于分类任务或者降维任务
    乳腺癌数据集		：load-barest-cancer（）：简单经典的用于二分类任务
    糖尿病数据集		：load-diabetes（）	：经典的用于回归任务
	【注意：这10个特征中的每个特征都已经被处理成0均值，方差归一化的特征值。】
    波士顿房价数据集		：load-boston（）	：经典的用于回归任务
    体能训练数据集		：load-linnerud（）	：经典的用于多变量回归任务。

鸢尾花数据集：
    是数据挖掘任务常用的一个数据集，鸢尾花数据集采集的是鸢尾花的测量数据以及其所属的类别。
测量数据包括：萼片长度、萼片宽度、花瓣长度、花瓣宽度。
类别共分为三类：Iris Setosa，Iris Versicolour，Iris Virginica。该数据集可用于多分类问题。

波士顿房价数据集：
    包含506组数据，每条数据包含房屋以及房屋周围的详细信息。其中包含城镇犯罪率、一氧化氮浓度、住宅平均房间数、到中心区域的加权距离以及自住房平均房价等。因此，波士顿房价数据集能够应用到回归问题上。

手写数字数据集：
    包括：1797个0-9的手写数字数据，每个数字由8*8大小的矩阵构成，矩阵中值的范围是0-16，代表颜色的深度。


2.可在线下载的数据集
    使用方法		：sklearn.datasets.fetch_<name>，一般规模较大
    fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,download_if_missing=True)
	【Olivetti 脸部图片数据集。】

3.计算机生成的数据集
    使用方法		：sklearn.datasets.make_<name>
    make_blobs			：多类单标签数据集，为每个类分配一个或多个正太分布的点集
    make_classification		：多类单标签数据集，为每个类分配一个或多个正太分布的点集
	【提供了为数据添加噪声的方式，包括维度相关性，无效特征以及冗余特征等】
    make_gaussian-quantiles	：将一个单高斯分布的点集划分为两个数量均等的点集，作为两类
    make_hastie-10-2		：产生一个相似的二元分类数据集，有10个维度
    make_circle			：产生二维二元分类数据集来测试某些算法的性能
    make_moom			：产生二维二元分类数据集来测试某些算法的性能
	【make_circle和make_moom可以为数据集添加噪声，可以为二元分类器产生一些球形判决界面的数据。】

4.svmlight/libsvm格式的数据集
    使用方法		：sklearn.datasets.load_svmlight_file(...)
	from sklearn.datasets import load_svmlight_file
	x_train,y_train = load_svmlight_file("/path/to/train_dataset.txt","")
		【如果要加在多个数据的时候，可以用逗号隔开】
    svmlight/libsvm的每一行样本的存放格式：
	<label><feature-id>:<feature-value>?<feature-id>:<feature-value>....

5.data.org在线下载获取的数据集
	使用方法	：sklearn.datasets.fetch_mldata(...)
	from sklearn.datasets.mldata import fetch_mldata
	import tempfile
	test_data_home = tempfile.mkdtemp()
	iris = fetch_mldata('iris', data_home=test_data_home)
	print(iris);print(iris.target.shape);print(iris.data.shape)





from sklearn.model_selection import train_test_split	# 分割数据集模块
用于将矩阵随机划分为训练子集和测试子集，并返回划分好的训练集测试集样本和训练集测试集标签。

格式：
X_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.3, random_state=0)

参数解释：

train_data	：被划分的样本特征集

train_target	：被划分的样本标签

test_size	：如果是浮点数，在0-1之间，表示样本占比；如果是整数的话就是样本的数量

random_state	：是随机数的种子。

随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。

随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：

种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。



from sklearn import tree
tree.export_graphviz()方法：设置决策树的图形
参数：	
decision_tree ： 决策树分类器
要导出到GraphViz的决策树。

out_file ： 文件对象或字符串，可选（默认=无）
输出文件的句柄或名称。如果None，结果以字符串形式返回。

max_depth ： int，optional（默认=无）
表示的最大深度。如果为None，则完全生成树。

feature_names ： 字符串列表，可选（默认=无）
每个功能的名称。

class_names ： 字符串列表，bool或None，可选（默认=无）
每个目标类的名称按升序排列。仅与分类相关且不支持多输出。如果True，则显示类名的符号表示。

label ： {'all'，'root'，'none'}，可选（默认='全'）
是否显示有关杂质的信息标签等。选项包括在每个节点显示的“all”，“root”仅显示在顶部根节点，或“none”不显示在任何节点。

filled ： bool，可选（默认= False）
设置为时True，绘制节点以指示用于分类的多数类，用于回归的值的极值或用于多输出的节点的纯度。

leaves_parallel ： bool，optional（默认值= False）
设置为时True，绘制树底部的所有叶节点。

impurity ： bool，可选（默认= True）
设置True为时，显示每个节点的杂质。

node_ids ： bool，optional（默认值= False）
设置为时True，显示每个节点上的ID号。

proportion ： bool，可选（默认= False）
设置为时True，将“值”和/或“样本”的显示分别更改为比例和百分比。

rotate ： bool，optional（默认值= False）
设置True为时，从左到右而不是从上到下定向树。

rounded ： bool，可选（默认= False）
设置为时True，绘制带圆角的节点框并使用Helvetica字体而不是Times-Roman。

special_characters ： bool，optional（默认值= False）
设置False为时，忽略PostScript兼容性的特殊字符。

precision ： int，optional（默认值= 3）
每个节点的杂质，阈值和值属性值中浮点精度的位数。

返回：	
dot_data ： string
GraphViz点格式的输入树的字符串表示形式。仅在out_file为None 时才返回。






特征工程：
数据预处理中的方法：

Fit()		:简单来说，就是求得训练集X的均值啊，方差啊，最大值啊，最小值啊这些训练集X固有的属性。
	（可以理解为一个训练过程）

Transform()	:在Fit的基础上，进行标准化，降维，归一化等操作
	（看具体用的是哪个工具，如PCA，StandardScaler等）

Fit_transform()	:fit_transform是fit和transform的组合，既包括了训练又包含了转换。

transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）

fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。

根据对之前部分trainData进行fit的整体指标，对剩余的数据（testData）使用同样的均值、方差、最大最小值等指标进行转换transform(testData)，从而保证train、test处理方式相同。







