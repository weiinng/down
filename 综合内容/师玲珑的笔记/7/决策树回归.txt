# 导入要用的数据库
from sklearn import datasets



# 先加载数据
housedata = datasets.load_boston()
# 特征
datasetX = housedata.data 
# 标签
datasetY = housedata.target 



# 分割数据 
# 要记住 这个模块
from sklearn.model_selection import train_test_split 
# 将测试集和训练集的x和y分开，test_size是测试集的大小
trainX,testX,trainY,testY = train_test_split(datasetsX,datasetY,test_size=0.2)



# 构建决策树模型 
# 决策树可做回归可做分类
from sklearn.tree import DecisionTreeRegressor
# max_depth 决策树深度
dtreeReg = DecisionTreeRegressor(max_depth=4)
# 拿训练集数据来训练模型
# dtreeReg.fit(trainX,trainY) 

dtreeReg.feature_importances_ #打印出每个特征所占的重要比例，这个数组里面的值相加等于1


# 预测数据
# 拿测试集的数据来预测数据
y_pre = dtreeReg.predict(testX)



# 绘制决策树
import graphviz
from sklearn import tree 
tree_data = tree.export_graphviz(dtreeReg)
# 取出数据
graph = graphviz.Source(tree_data)
# view = True打开pdf
graph.render('tree.pdf',view=True)




# 评测模型
from sklearn import metrics
print('平均绝对误差',metrics.mean_absolute_error(y_pred = test_y_pre,y_true=testY))
print('均方误差',metrics.mean_squared_error(y_pred=test_y_pre,y_true=testY))
print('解释方差分R2',metrics.r2_score(y_pred=test_y_pre,y_true=testY))


信息熵的值越高则表示当前数据无序程度越高，越低则表示数据无序程度越低。对于数据分类，我们当然希望得到无序程度尽可能低的分类方式！

那么，每选择一个分类特征，该数据的无序程度应该得到一定的减少，而这个减少的量我们称之为信息增益！

#对决策数进行调优
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
# n_estimators=400 ，允许生成最多400颗树
dtreeReg = DecisionTreeRegressor(max_depth=4)
adaboostRe = AdaBoostRegressor(dtreeReg,n_estimators=400)
adaboostRe.fit(trainx,trainy) #不在初始化决策树时训练，而是在初始化adaboostRe时训练
predict_test_y = adaboostRe.predict(testx)

import sklearn.metrics as metrics 
print('决策树回归模型的评测结果- ---->>>') 
print('均方误差MSE：{}'.format(round(metrics.mean_squared_error(predict_test_y,testy),2))) 
print('解释方差分：{}'.format(round(metrics.explained_variance_score(predict_test_y,testy),2)))

还可以修改树的深度 max_depth
