'''
朴素贝叶斯的优点在于：
1，有稳定的分类效率，
2，对小规模数据表现很好，能处理多分类任务，适合增量式训 练，尤其是数据量超出内存时，可以一批一批的去增量训练。
3，对缺失数据不太敏感，算法比较简单，常用于文 本分类。
但朴素贝叶斯的缺点是：
1，朴素贝叶斯算法有一个重要的使用前提：
样本的特征属性之间是相互独立的，这使得 朴素贝叶斯算法在满足这一条件的数据集上效果非常好，而不满足独立性条件的数据集上，效果欠佳。理论上，朴 素贝叶斯模型与其他分类方法相比，有最小的误差率，但是这一结果仅限于满足独立性条件的数据集上。在实际应 用中，属性之间不太可能完全独立，特别是在特征属性个数非常多，且属性之间相关性较大时，朴素贝叶斯分类效 果不太好。
2，需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候 会由于假设的先验模型的原因导致预测效果不佳。
3，由于通过先验和数据来决定后验的概率从而决定分类，所以 分类决策存在一定的错误率。

'''
from sklearn.naive_bayes import GaussianNB
gaussianNB_new=GaussianNB()
gaussianNB_new.fit(train_X,train_Y)

#用交叉验证来验证模型的准确性，只是在test set上验证准确性
from sklearn.model_selection import cross_val_score
cv=5  #将原始数据分成5份进行验证，scoring:调用方法
#分数只有在分类器从未见过的数据上计算时才有意义
accuracy=cross_val_score(gaussianNB_new,test_X,test_Y,scoring='accuracy',cv=cv)
print('准确率{:.2f}%'.format(accuracy.mean()*100))
precision=cross_val_score(gaussianNB_new,test_X,test_Y,scoring='precision_weighted',cv=cv) 
print('精确度：{:.2f}%'.format(precision.mean()*100))
recall=cross_val_score(gaussianNB_new,test_X,test_Y,scoring='recall_weighted',cv=cv) 
print('召回率：{:.2f}%'.format(recall.mean()*100)) 
f1=cross_val_score(gaussianNB_new,test_X,test_Y,scoring='f1_weighted',cv=cv)
print('F1  值：{:.2f}%'.format(f1.mean()*100))

