归一化：对原始数据进行线性变换把数据映射到[-1,1]之间，便于不同量纲的指标能够进行比较，它会丢失一些信息，特别对于那些异常的点
标准化：均值为0，标准差为1（标准正太分布）


――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――

均值移除(中心化)：均值为0，标准差为1，避免特征中一家独大
范围缩放：缩放到指定区间，作用是对方差非常小的特征增强其稳定性

+――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――


二值化：0、1，适用于某种特征出现与否的场合


――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――


独热编码(又称一位有效编码，哑变量)：用于可处理非数字的特征值，作用是将离散特征的某个取值就对应欧式空间的某个点，特征之间的距离计算更加合理。


――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――

标签编码：将离散型变量或文本进行转换成连续的数值型变量，即对不连续的数字或者文本进行编号。

――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――




标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。



什么时候用归一化？什么时候用标准化？
??
（1）如果对输出结果范围有要求，用归一化。
?
?（2）如果数据较为稳定，不存在极端的最大最小值，用归一化。

（3）如果数据存在异常值和较多噪音，用标准化，（可以间接通过中心化避免异常值和极端值的影响）
        
 ( 4) 需要使用距离来度量样本相似性的时候，使用标准化，（可以间接通过中心化避免异常值和极端值的影响）
       
 5）树算法中，不需要进行归一化和标准化



什么时候(不)用独热编码？
	
（1）使用：独热编码用来解决类别型数据的离散值问题
	（2）不用：树算法中，不用计算特征间距离，所以不用独热


独热编码优点：能够处理非连续型数值特征，


独热编码缺点：当类别的数量很多时，特征空间会变得非常大，稀疏矩阵会很稀，占内存空间大。一般可以用PCA来减少维度。
Label encoding缺点：比如有[dog,cat,dog,mouse,cat]，我们把其转换为[1,2,1,3,2]。这里就产生了一个奇怪的现象：dog和mouse的平均值是cat


均值移除
#如果有的列，一列的值最大值和最小值相差很大，
# 就要考虑使用均值移除，将数据大小相差不大差

#目的是为了再跑模型时效率快，
# 数据结果大小范围波动不大，找这组数据的规律
from sklearn import preprocessing  #预处理模块
import numpy as np

data=np.array([[1000,10,40],[10,300,10],[20,1,30]])
result=preprocessing.scale(data)  #0均值处理，就是把所有数据都变得特别小
print(result)
print(np.mean(result,axis=0))  #每列的平均值约等于0
print(np.std(result,axis=0))  #约等于1，还是每列


分词

import jieba
jieba.load_userdict('1.txt')   #导入词典
ret = jieba.lcut('你吃饭啦吗吃的什么',cut_all=True)
print(ret)
result = jieba.lcut('你吃饭啦吗吃的什么',cut_all=False)
print(result)
res = jieba.lcut_for_search('你可真是个小天才啊小马')
print(res)
re = jieba.lcut('无妻徒形今天',cut_all=False)
print(re)

'''
范围缩放：
将数据压缩到一个你指定的区间里，
相比于均值移除，它的处理结果方差还是存在的，可以分析数据波动

必要性：数据点中每个特征列的数值范围可能变化很大，因此，有时需要将特征列的数值范围缩放到合理的大小

import numpy as np from sklearn import preprocessing
 
data=np.array([[3, -1.5, 2, -5.4],               [0, 4,-0.3,2.1],               [1, 3.3, -1.9, -4.3]]) # 原始数据矩阵 shape=(3,4)
 
data_scaler=preprocessing.MinMaxScaler(feature_range=(0,1)) # 缩放到（0,1）之间 

data_scaled=data_scaler.fit_transform(data) 
print('scaled matrix: *********************************') 
print(data_scaled)

scaled matrix: ************* [[1. 0. 1. 0. ] [0. 1. 0.41025641 1. ] [0.33333333 0.87272727 0. 0.14666667]] 


均值移除：
每列均值约为0，方差约为1

范数处理：
你要处理的数据集合的L1范数的数据的绝对值之和为1
你要处理的数据集合的L2范数的数据的平方之和为1，他的数据更敏感

独热编码：
相当于对数字进行编码
在矩阵中，若数值为0的元素数目远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵；
与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵。
通常，需要处理的数值都是稀疏地，散乱地分布在空间中，但我们并不需要存储这些大数值，这时就需要使用独热 编码，独热编码实际上是一种收紧特征向量的工具

from sklearn import preprocessing
 
data=np.array([[0,2,1,12],               [1,3,5,3],               [2,3,2,12],               [1,2,4,3]]) # 原始数据矩阵 shape=(4,4)
 
encoder=preprocessing.OneHotEncoder() 
encoder.fit(data) 
encoded_vector=encoder.transform([[2,3,5,3]]).toarray() 
print('one-hot encoded matrix: *********************************') 
print(encoded_vector.shape) print(encoded_vector)

结果为：
one-hot encoded matrix: ************* (1, 11) [[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]] 



_______________________________________________________________________________________

二值化用于将数值特征向量转换为布尔类型向量。
###########对数据集进行Binarization######################### import numpy as np from sklearn import preprocessing
 
data=np.array([[3, -1.5, 2, -5.4],               [0, 4,-0.3,2.1],               [1, 3.3, -1.9, -4.3]]) # 原始数据矩阵 shape=(3,4)
 
data_binarized=preprocessing.Binarizer(threshold=1.4).fit_transform(data) 
#小于等于 threshold 为 0 
print('binarized matrix: *********************************') 
print(data_binarized)

结果为;
binarized matrix: ************* [[1. 0. 1. 0.] [0. 1. 0. 1.] [0. 1. 0. 0.]] 


，二值化之后的数据点都是0或者1，所以叫做二值化。
2，计算方法是，将所有大于threshold的数据都改为1，小于等于threshold的都设为0。 3，经常用于出现某种特征（比如设为1），或者没有出现某种特征（设为0）的应用场合。 

'''
――――――――――――――――――――――――――――――――――――――――――

范数：
import numpy as np
from sklearn import preprocessing
data=np.array([[3,-1.5,2,-5.4],[3,-1.5,2,-5.4],[3,-1.5,2,-5.4]])
data_l1_hang=preprocessing.normalize(data,norm='l1',axis=1) #norm是代表L几数
print(data_l1_hang)
data_l1_lie=preprocessing.normalize(data,norm='l1',axis=0)
print(data_l1_lie)




